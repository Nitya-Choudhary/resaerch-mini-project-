@article{Raza2022,
   abstract = {Fake news is a real problem in today’s world, and it has become more extensive and harder to identify. A major challenge in fake news detection is to detect it in the early phase. Another challenge in fake news detection is the unavailability or the shortage of labelled data for training the detection models. We propose a novel fake news detection framework that can address these challenges. Our proposed framework exploits the information from the news articles and the social contexts to detect fake news. The proposed model is based on a Transformer architecture, which has two parts: the encoder part to learn useful representations from the fake news data and the decoder part that predicts the future behaviour based on past observations. We also incorporate many features from the news content and social contexts into our model to help us classify the news better. In addition, we propose an effective labelling technique to address the label shortage problem. Experimental results on real-world data show that our model can detect fake news with higher accuracy within a few minutes after it propagates (early detection) than the baselines.},
   author = {Shaina Raza and Chen Ding},
   doi = {10.1007/s41060-021-00302-z},
   issn = {23644168},
   issue = {4},
   journal = {International Journal of Data Science and Analytics},
   keywords = {Concept drift,Fake news,Social contexts,Transformer,User credibility,Weak supervision,Zero shot learning},
   month = {5},
   pages = {335-362},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Fake news detection based on news content and social contexts: a transformer-based approach},
   volume = {13},
   year = {2022}
}
@article{Baashirah2024,
   abstract = {The propagation of fake news has significant societal and economic impacts, particularly in the context of the exponential growth of digital and social media platforms. Traditional Machine Learning (ML) and Deep Learning (DL) models, i.e., Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN), and Long Short-Term Memory (LSTM) networks, have been employed to address Fake News Detection (FND) with varying degrees of success. However, these models often struggle with data dependency, overfitting, scalability, flexibility, and high computational costs. A Zero-Shot Learning (ZSL) approach is proposed for Fake News Detection (ZS-FND) to address these challenges. ZSL models can predict outcomes with limited training data and do not rely on domain-specific labeled data, making them well-suited for handling fake news's diverse and evolving nature. The proposed ZS-FND model leverages the pre-trained Bidirectional Encoder Representations from Transformers (BERT) for the semantic representation of textual data and generates word vectors. ZSL considers such vectors as input for FND. The experimental results demonstrate that ZS-FND outperforms conventional ML and DL models, achieving 98.39% accuracy, 97.33% precision, 95.67% recall, 96.49% F1-score, and 0.0160 Mean Absolute Error (MAE). ZS-FND improves accuracy, precision, recall, and F1-score by 10.76%, 4.05%, 5.96%, and 5.01%, respectively. These findings highlight the potential of ZSL models in providing a more robust and efficient solution for FND.},
   author = {Rania Baashirah},
   doi = {10.1109/ACCESS.2024.3462151},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {BERT,Classification,Deep Learning,Fake News Detection,Zero-Shot Classifier},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Zero-Shot Automated Detection of Fake News: An Innovative Approach (ZS-FND)},
   year = {2024}
}
@article{Park2023,
   abstract = {As fake news spreads rapidly in social media, attempts to develop detection technology to automatically identify fake news are actively being developed, recently. However, most of them focus only on the linguistic and compositional characteristics of fake news (e.g., source or authors indication, length of a message, frequency of negative words). Compared to them, this study proposes a fake news detection model based on machine learning that reflects the characteristics of users, news content, and social networks based on social capital. To comprehensively reflect the characteristics related to the spread of fake news, this study applied the XGBoost model to estimate the feature importance of each variable to derive the priority factors that preferentially affect fake news detection. Based on the derived variables, we established SVM, RF, LR, CART, and NNET, which are representative classification models of machine learning, and compared the performance rate of fake news detection. To generalize the established models (i.e., to avoid overfitting or underfitting), this study performed a cross-validation step, and to compare the predictive accuracy of the established models. As a result, the RF model indicated the highest prediction rate at about 94%, while the NNET had the lowest performance rate at about 92.1%. The results of this study are expected to contribute to improve the fake news detection system in preparation for the more sophisticated generation and spread of fake news.},
   author = {Minjung Park and Sangmi Chai},
   doi = {10.1109/ACCESS.2023.3294613},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Classification algorithms,XGBoost,fake news,fake news detection,feature selection,prediction algorithms,predictive models},
   pages = {71517-71527},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Constructing a User-Centered Fake News Detection Model by Using Classification Algorithms in Machine Learning Techniques},
   volume = {11},
   year = {2023}
}
@article{Das2021,
   abstract = {The significance of social media has increased manifold in the past few decades as it helps people from even the most remote corners of the world to stay connected. With the advent of technology, digital media has become more relevant and widely used than ever before and along with this, there has been a resurgence in the circulation of fake news and tweets that demand immediate attention. In this paper, we describe a novel Fake News Detection system that automatically identifies whether a news item is "real" or "fake", as an extension of our work in the CONSTRAINT COVID-19 Fake News Detection in English challenge. We have used an ensemble model consisting of pre-trained models followed by a statistical feature fusion network , along with a novel heuristic algorithm by incorporating various attributes present in news items or tweets like source, username handles, URL domains and authors as statistical feature. Our proposed framework have also quantified reliable predictive uncertainty along with proper class output confidence level for the classification task. We have evaluated our results on the COVID-19 Fake News dataset and FakeNewsNet dataset to show the effectiveness of the proposed algorithm on detecting fake news in short news content as well as in news articles. We obtained a best F1-score of 0.9892 on the COVID-19 dataset, and an F1-score of 0.9073 on the FakeNewsNet dataset.},
   author = {Sourya Dipta Das and Ayan Basak and Saikat Dutta},
   month = {4},
   title = {A Heuristic-driven Uncertainty based Ensemble Framework for Fake News Detection in Tweets and News Articles},
   url = {http://arxiv.org/abs/2104.01791},
   year = {2021}
}
@article{Zhou2020,
   abstract = {Effective detection of fake news has recently attracted significant attention. Current studies have made significant contributions to predicting fake news with less focus on exploiting the relationship (similarity) between the textual and visual information in news articles. Attaching importance to such similarity helps identify fake news stories that, for example, attempt to use irrelevant images to attract readers' attention. In this work, we propose a $\mathsf\{S\}$imilarity-$\mathsf\{A\}$ware $\mathsf\{F\}$ak$\mathsf\{E\}$ news detection method ($\mathsf\{SAFE\}$) which investigates multi-modal (textual and visual) information of news articles. First, neural networks are adopted to separately extract textual and visual features for news representation. We further investigate the relationship between the extracted features across modalities. Such representations of news textual and visual information along with their relationship are jointly learned and used to predict fake news. The proposed method facilitates recognizing the falsity of news articles based on their text, images, or their "mismatches." We conduct extensive experiments on large-scale real-world data, which demonstrate the effectiveness of the proposed method.},
   author = {Xinyi Zhou and Jindi Wu and Reza Zafarani},
   month = {2},
   title = {SAFE: Similarity-Aware Multi-Modal Fake News Detection},
   url = {http://arxiv.org/abs/2003.04981},
   year = {2020}
}
@article{Khanam2021,
   abstract = {The fake news on social media and various other media is wide spreading and is a matter of serious concern due to its ability to cause a lot of social and national damage with destructive impacts. A lot of research is already focused on detecting it. This paper makes an analysis of the research related to fake news detection and explores the traditional machine learning models to choose the best, in order to create a model of a product with supervised machine learning algorithm, that can classify fake news as true or false, by using tools like python scikit-learn, NLP for textual analysis. This process will result in feature extraction and vectorization; we propose using Python scikit-learn library to perform tokenization and feature extraction of text data, because this library contains useful tools like Count Vectorizer and Tiff Vectorizer. Then, we will perform feature selection methods, to experiment and choose the best fit features to obtain the highest precision, according to confusion matrix results.},
   author = {Z Khanam and B N Alwasel and H Sirafi and M Rashid},
   doi = {10.1088/1757-899x/1099/1/012040},
   issn = {1757-8981},
   issue = {1},
   journal = {IOP Conference Series: Materials Science and Engineering},
   month = {3},
   pages = {012040},
   publisher = {IOP Publishing},
   title = {Fake News Detection Using Machine Learning Approaches},
   volume = {1099},
   year = {2021}
}
@techReport{,
   abstract = {In the digital age, the spread of false information has become a widespread and difficult problem. The Naive Bayes & logistic regression algorithms are used in this paper to provide a novel methodology for the detection of bogus news stories. The aim of this study is to improve the efficacy of the identification of fake news in digital material, consequently fostering information credibility and integrity within the digital ecosystem. We start this investigation by gathering a wide dataset of news articles from both reputable and phoney sources. We preprocess the textual input using techniques like tokenization, stop-word removal, and stemming to aid in feature extraction. During the feature selection phase, the term frequency-inverse document frequency (TF-IDF) is used to estimate the word importance of each article. Next, the Naive Bayes algorithm is used to divide news stories into two groups: phoney and real. In order to determine the probability that an article will fall into a particular category, Naive Bayes uses a probabilistic technique under the assumption that the characteristics (words) are conditionally independent. Logistic Regression models the probability of a news article being fake or genuine based on a set of relevant textual features. The primary goal of logistic regression is to achieve high accuracy in classifying news articles as fake or genuine, with an emphasis on feature engineering and model evaluation. The efficacy of the corresponding methods is determined by utilizing the confusion matrix to evaluate the correctness of the model. The findings suggest that Logistic Regression is effective in detecting fake news and contributes to the trustworthiness of information sources in the digital age.},
   author = {Mr Vyankatesh Rampurkar and D R Thirupurasundari},
   issue = {3},
   journal = {Original Research Paper International Journal of Intelligent Systems and Applications in Engineering IJISAE},
   keywords = {Confusion Matrix,ISOT Dataset,Logistic Regression,Machine Learning,Navie Bayes},
   pages = {2868-2874},
   title = {International Journal of INTELLIGENT SYSTEMS AND APPLICATIONS IN ENGINEERING An Approach towards Fake News Detection using Machine Learning Techniques},
   volume = {2024},
   url = {www.ijisae.org}
}
@article{Singh2023,
   abstract = {It is difficult to estimate the exact percentage of news that is fake nowadays as it varies depending on the source and type of news. However, studies have shown that the jump of fake news is a significant problem nowadays due to the ease of creation and sharing information on publicly accessible online platforms. Social media platforms play a significant role in exacerbating this problem either directly or indirectly, with algorithms that prioritize engagement and the sharing of sensational content contributing to the problem. In addition, political polarization and the rise of disinformation campaigns have further exacerbated the spread of fake news. While there are constant efforts to combat the spread of fake news, it remains a pervasive issue that requires continued attention and vigilance.},
   author = {Awanttika Singh and Drishti Parijat and Shivani Kumari and Rachana D R and Asst Prof Prashanth},
   issn = {2349-6002},
   keywords = {Fake news,Social media,algorithms},
   title = {IJIRT 158993 INTERNATIONAL JOURNAL OF INNOVATIVE RESEARCH IN TECHNOLOGY 130 Fake News Detection Using Machine Learning},
   year = {2023}
}
@article{Wang2024,
   abstract = {Fake news detection is crucial for preventing the spread of misinformation on social media. Whereas existing researches tend to ignore some of the hidden signals in the news environments, which verify the authenticity of the news posts by zooming in textual signals and incorporating external knowledge. In this study, we propose the Spectral clustering Environments and data Augmentation for Fake News Detection (SEAFND) method, which provides novelty and popularity from the news posts for fake news detection to boost the detection accuracy. We first leverage the graph theory-based spectral clustering approach to obtain the center-of-mass vectors and entropies to capture the complex associations and implicit evidences among the news on social media. Then, we introduce a shared parameter multitask learning framework that treats different news environments as independent tasks and architects GRU bootstrapping modules with attention mechanisms to help aggregate features from different environments efficiently and interpretably. Finally, we provide textual perspective and stylistic perspective approaches during the detection process and soften the loss terms in multiple environments to alleviate the strict constraints, thus making it more compatible with the fake news detection task. Compared with the state-of-the-art methods, the SEAFND method improves the detection performance by 1%-2% on the multidomain datasets Ch-9 and En-3.},
   author = {Kun Wang and Yuzhen Yang and Xiaoyang Wang},
   doi = {10.1109/ACCESS.2024.3521015},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Fake news detection,data argument,knowledge sources,news environment,social context},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Spectral Clustering-Guided News Environments Perception for Fake News Detection},
   year = {2024}
}
@article{Gururaj2022,
   abstract = {In the modern era where the internet is found everywhere and there is rapid adoption of social media which has led to the spread of information that was never seen within human history before. This is due to the usage of social media platforms where consumers are creating and sharing more information where most of them are misleading with no relevance with reality. Classifying the text article automatically as misinformation is a bit challenging task. This development addresses how automated classification of text articles can be done. We use a machine learning approach for the classification of news articles. Our study involves exploring different textual properties that may be often used to distinguish fake contents from real ones. By using those properties, can train the model with different machine learning algorithms and evaluate their performances. The classifier with the best performance is used to build the classification model which predicts the reliability of the news articles present in the dataset.},
   author = {H. L. Gururaj and H. Lakshmi and B. C. Soundarya and Francesco Flammini and V. Janhavi},
   doi = {10.13052/jicts2245-800X.1042},
   issn = {22460853},
   issue = {4},
   journal = {Journal of ICT Standardization},
   keywords = {Fake news,classification,machine learning},
   pages = {509-530},
   publisher = {River Publishers},
   title = {Machine Learning-Based Approach for Fake News Detection},
   volume = {10},
   year = {2022}
}
@article{Altheneyan2023,
   abstract = {Users rely heavily on social media to consume and share news, facilitating the mass dis-semination of genuine and fake stories. The proliferation of misinformation on various social media platforms has serious consequences for society. The inability to differentiate between the several forms of false news on Twitter is a major obstacle to effective detection of fake news. Researchers have made progress toward a solution by emphasizing methods for identifying fake news. The dataset FNC-1, which includes four categories for identifying false news, will be used in this study. The state-of-the-art methods for spotting fake news are evaluated and compared using big data technology (Spark) and machine learning. The methodology of this study employed a decentralized Spark cluster to create a stacked ensemble model. Following feature extraction using N-grams, Hashing TF-IDF, and count vectorizer, we used the proposed stacked ensemble classification model. The results show that the suggested model has a superior classification performance of 92.45% in the F1 score compared to the 83.10 % F1 score of the baseline approach. The proposed model achieved an additional 9.35% F1 score compared to the state-of-the-art techniques.},
   author = {Alaa Altheneyan and Aseel Alhadlaq},
   doi = {10.1109/ACCESS.2023.3260763},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Big data,ensemble learning,fake news,machine learning,social media},
   pages = {29447-29463},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Big Data ML-Based Fake News Detection Using Distributed Learning},
   volume = {11},
   year = {2023}
}
@article{Koru2024,
   abstract = {As the number of people using social networks increases, more people are using social media platforms to meet their news needs. Users think that it is easier to follow the agenda by accessing news, especially on Twitter, rather than newspaper news pages. However, fake news is increasingly appearing on social media, and it is not always possible for people to obtain correct news from partial news pages or short Twitter posts. Understanding whether the news shared on Twitter is true or not is an important problem. Detecting fake tweets is of great importance in Turkish as well as in any language. In this study, fake news obtained from verification platforms on Twitter and real news obtained from the Twitter accounts of mainstream newspapers were labeled and, preprocessed using the Zemberek natural language processing tool developed for the Turkish language, and a dataset named TR_FaRe_News was created. Then, the TR_FaRe_News dataset was explored using ensemble methods and BoW, TF-IDF, and Word2Vec vectorization methods for fake news detection. Then a pre-trained BERT deep learning model was fine-tuned, and variations of the model extended with Bi-LSTM and Convolutional Neural Network (CNN) layers with the frozen and unfrozen parameters methods were explored. The performance evaluation was conducted using seven comparable datasets, namely BuzzFeedNews, GossipCop, ISOT, LIAR, Twitter15, and Twitter16, including an LLM-generated fake news dataset. Analyzing Turkish tweets and using fake news datasets generated by LLM is considered an important contribution. Accuracy values between 90 and 94% were obtained with the BERT and BERTurk + CNN models with 94% accuracy.},
   author = {Gulsum Kayabasi Koru and Celebi Uluyol},
   doi = {10.1109/ACCESS.2024.3354165},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {BERT,Fake news,deep learning,ensemble learning,generated news},
   pages = {14918-14931},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Detection of Turkish Fake News from Tweets with BERT Models},
   volume = {12},
   year = {2024}
}
@article{Villela2023,
   abstract = {Fake news (i.e., false news created to have a high capacity for dissemination and malicious intentions) is a problem of great interest to society today since it has achieved unprecedented political, economic, and social impacts. Taking advantage of modern digital communication and information technologies, they are widely propagated through social media, being their use intentional and challenging to identify. In order to mitigate the damage caused by fake news, researchers have been seeking the development of automated mechanisms to detect them, such as algorithms based on machine learning as well as the datasets employed in this development. This research aims to analyze the machine learning algorithms and datasets used in training to identify fake news published in the literature. It is exploratory research with a qualitative approach, which uses a research protocol to identify studies with the intention of analyzing them. As a result, we have the algorithms Stacking Method, Bidirectional Recurrent Neural Network (BiRNN), and Convolutional Neural Network (CNN), with 99.9%, 99.8%, and 99.8% accuracy, respectively. Although this accuracy is expressive, most of the research employed datasets in controlled environments (e.g., Kaggle) or without information updated in real-time (from social networks). Still, only a few studies have been applied in social network environments, where the most significant dissemination of disinformation occurs nowadays. Kaggle was the platform identified with the most frequently used datasets, being succeeded by Weibo, FNC-1, COVID-19 Fake News, and Twitter. For future research, studies should be carried out in addition to news about politics, the area that was the primary motivator for the growth of research from 2017, and the use of hybrid methods for identifying fake news.},
   author = {Humberto Fernandes Villela and Fábio Corrêa and Jurema Suely de Araújo Nery Ribeiro and Air Rabelo and Dárlinton Barbosa Feres Carvalho},
   doi = {10.5753/jis.2023.3020},
   issn = {27637719},
   issue = {1},
   journal = {Journal on Interactive Systems},
   month = {3},
   pages = {47-58},
   publisher = {Sociedade Brasileira de Computacao - SB},
   title = {Fake news detection: a systematic literature review of machine learning algorithms and datasets},
   volume = {14},
   year = {2023}
}
@article{Yuan2019,
   abstract = {The development of social media has revolutionized the way people communicate, share information and make decisions, but it also provides an ideal platform for publishing and spreading rumors. Existing rumor detection methods focus on finding clues from text content, user profiles, and propagation patterns. However, the local semantic relation and global structural information in the message propagation graph have not been well utilized by previous works. In this paper, we present a novel global-local attention network (GLAN) for rumor detection, which jointly encodes the local semantic and global structural information. We first generate a better integrated representation for each source tweet by fusing the semantic information of related retweets with the attention mechanism. Then, we model the global relationships among all source tweets, retweets, and users as a heterogeneous graph to capture the rich structural information for rumor detection. We conduct experiments on three real-world datasets, and the results demonstrate that GLAN significantly outperforms the state-of-the-art models in both rumor detection and early detection scenarios.},
   author = {Chunyuan Yuan and Qianwen Ma and Wei Zhou and Jizhong Han and Songlin Hu},
   month = {9},
   title = {Jointly embedding the local and global relations of heterogeneous graph for rumor detection},
   url = {http://arxiv.org/abs/1909.04465},
   year = {2019}
}
@techReport{,
   abstract = {In our modern era where the internet is ubiquitous, everyone relies on various online resources for news. Along with the increase in the use of social media platforms like Facebook, Twitter, etc. news spread rapidly among millions of users within a very short span of time. The spread of fake news has far-reaching consequences like the creation of biased opinions to swaying election outcomes for the benefit of certain candidates. Moreover, spammers use appealing news headlines to generate revenue using advertisements via click-baits. In this paper, we aim to perform binary classification of various news articles available online with the help of concepts pertaining to Artificial Intelligence, Natural Language Processing and Machine Learning. We aim to provide the user with the ability to classify the news as fake or real and also check the authenticity of the website publishing the news.},
   author = {Uma Sharma and Sidarth Saran and Shankar M Patil},
   keywords = {Artificial Intelligence,Authenticity,Classification,Fake News,Internet,Machine Learning,Social Media,Websites},
   title = {Fake News Detection using Machine Learning Algorithms},
   url = {www.ijert.org}
}
@techReport{,
   abstract = {This paper explores the importance of business intelligence with a focus on leveraging sales data visualization to gather strategic insights. Beginning with an examination of the importance of data analysis and visualization of sales data, the study emphasizes its crucial role in identifying trends, optimizing performance, and making strategic decisions. Furthermore, the paper offers valuable insights into effective visualizations using the US regional sales dataset which encompasses of sales, transactions and customer data which serves as a rich resource for analyzing sales patterns, product popularity, and channel performance. Practical ideas for visualizing this dataset are presented through this study, catering to a broad audience seeking actionable intelligence from their sales data. Therefore, this paper synthesizes key findings, addresses limitations, and emphasizes the complex nature of sales data and how visualization can empower businesses to harness the full potential of their sales data, fostering data-driven decision-making in an ever-evolving marketplace.},
   author = {Akshata Upadhye},
   issue = {1},
   journal = {International Journal of Data Science Research and Development (IJDSRD)},
   keywords = {Business Intelligence,Data Analysis,Data-driven Decision-making,Sales Data Visualization,Sales Trends,Visual Analytics},
   pages = {27-36},
   title = {Article ID: IJDSRD_02_01_004 Cite this Article: Akshata Upadhye, Enhancing business strategy with sales data visualization},
   volume = {2},
   url = {https://iaeme.com/Home/issue/IJDSRD?Volume=2&Issue=1}
}
@article{Windrasari2025,
   abstract = {<p>The coffee shop industry has experienced significant growth, evolving into a highly competitive marketplace demanding specialty coffee and personalized experiences. While data-driven strategies are crucial for optimizing operations, many owners still struggle to effectively leverage their sales data to understand dynamic customer behavior and enhance decision-making. Addressing this gap, this study explores the application of machine learning (ML) techniques, specifically the Random Forest Regressor model, to predict sales performance within the coffee shop business environment. By analyzing factors such as transaction timing, store location, product type, and day of the week, this research aims to uncover patterns that can enhance inventory management and customer engagement. The Random Forest model was evaluated through cross-validation, yielding a mean Mean Squared Error (MSE) of 80.97, which indicates moderate predictive accuracy and represents an improvement over traditional forecasting methods commonly employed in the industry. Feature importance analysis revealed that Premium Beans is the most influential predictor, followed by seasonal trends (month), time of day, and weekend sales patterns. These findings underscore the importance of incorporating temporal and contextual factors into forecasting models. </p>},
   author = {Shella Norma Windrasari and Hendro Margono and Yudistira Ardi Nugraha Setyawan Putra},
   doi = {10.57152/malcom.v5i3.2023},
   issn = {2775-8575},
   issue = {3},
   journal = {MALCOM: Indonesian Journal of Machine Learning and Computer Science},
   month = {7},
   pages = {1000-1011},
   title = {Predictive Sales Analysis in Coffee Shops Using the Random Forest Algorithm},
   volume = {5},
   url = {https://journal.irpi.or.id/index.php/malcom/article/view/2023},
   year = {2025}
}
@techReport{,
   author = {Anoop Krishna Kristu and Jayanti College},
   title = {A REPORT ON ANALYSIS OF COFFEE SALES},
   url = {https://www.researchgate.net/publication/388841961}
}
